# ML Модели - Подробная документация

## Содержание

1. [Обзор](#обзор)
2. [Используемые модели](#используемые-модели)
3. [Датасеты и обучение](#датасеты-и-обучение)
4. [Механизм загрузки моделей](#механизм-загрузки-моделей)
5. [Принцип работы](#принцип-работы)
6. [Конфигурация](#конфигурация)
7. [Производительность и оптимизация](#производительность-и-оптимизация)

---

## Обзор

Система использует предобученные модели из библиотеки Hugging Face для выполнения двух основных задач:

1. **Парафразирование** - переписывание текста с сохранением смысла
2. **Суммаризация** - сокращение длинных текстов до ключевой информации

Все модели загружаются автоматически при первом использовании (lazy loading) и кэшируются локально для ускорения последующих запусков.

---

## Используемые модели

### Модели для парафразирования

#### 1. cointegrated/rut5-base-paraphraser (Русский язык)

**Описание:**
- Архитектура: T5 (Text-To-Text Transfer Transformer)
- Размер: ~250 MB
- Язык: Русский
- Назначение: Парафразирование русскоязычных текстов

**Характеристики:**
- Базовая модель: T5-base
- Fine-tuned на датасете русских синонимичных пар
- Сохраняет семантический смысл при изменении формулировок
- Работает без дополнительных промптов

**Использование:**
```python
# Автоматически определяется язык и используется соответствующая модель
paraphrased = await text_processor.paraphrase(text="Исходный текст")
```

**Параметры генерации:**
- `max_length`: 512 токенов
- `temperature`: 0.7 (баланс между разнообразием и качеством)
- `top_p`: 0.9 (nucleus sampling)
- `num_beams`: 5 (beam search для лучшего качества)

#### 2. google/flan-t5-large (Английский язык)

**Описание:**
- Архитектура: T5-Large
- Размер: ~780 MB
- Язык: Английский
- Назначение: Парафразирование англоязычных текстов

**Характеристики:**
- Fine-tuned версия T5 с улучшенной инструкционной настройкой (FLAN)
- Более качественные результаты за счет инструкционного обучения
- Требует префикс "paraphrase:" перед текстом

**Использование:**
```python
# Система автоматически добавляет префикс для английской модели
prompt = f"paraphrase: {text}"
```

**Особенности:**
- Более точное сохранение смысла
- Лучшая работа с формальными текстами
- Требует больше памяти (Large вместо Base)

### Модели для суммаризации

#### 1. IlyaGusev/mbart_ru_sum_gazeta (Русский язык)

**Описание:**
- Архитектура: mBART (Multilingual BART)
- Размер: ~2-3 GB
- Язык: Русский
- Назначение: Суммаризация русскоязычных новостных текстов

**Характеристики:**
- Обучена на датасете Gazeta (российские новости)
- Специализирована на новостном контенте
- Высокое качество суммаризации
- Сохраняет ключевую информацию

**Датасет обучения:**
- **Gazeta** - датасет российских новостей с ручными суммариями
- Более 90,000 пар статья-суммари
- Разнообразие тематик: политика, экономика, спорт, культура и т.д.

**Использование:**
```python
summary = await text_processor.summarize(
    text="Длинный текст статьи...",
    target_length=600  # Желаемая длина в символах
)
```

**Параметры генерации:**
- `max_length`: рассчитывается динамически на основе `target_length`
- `min_length`: 30% от `max_length` (минимум 50 токенов)
- `num_beams`: 4 (баланс между качеством и скоростью)
- `length_penalty`: 1.2 (поощряет более короткие суммари)
- `no_repeat_ngram_size`: 3 (избегает повторений)

**Особенности:**
- Медленная загрузка при первом запуске (модель большая)
- Требует значительной памяти (рекомендуется 4GB+ RAM)
- Высокое качество для новостного контента

#### 2. facebook/bart-large-cnn (Английский язык)

**Описание:**
- Архитектура: BART-Large
- Размер: ~1.6 GB
- Язык: Английский
- Назначение: Суммаризация англоязычных текстов

**Характеристики:**
- Обучена на датасете CNN/DailyMail
- Fine-tuned для extractive-abstractive суммаризации
- Отличное качество для новостных текстов

**Датасет обучения:**
- **CNN/DailyMail** - новостные статьи с суммариями
- ~300,000 пар статья-суммари
- Актуальные новости различных тематик

---

## Датасеты и обучение

### Датасеты для парафразирования

#### Русский язык (RUT5)

Модель `cointegrated/rut5-base-paraphraser` обучена на:
- Синонимичных парах русских предложений
- Парафразах различных стилей (формальный, неформальный)
- Разнообразном корпусе русскоязычных текстов

#### Английский язык (FLAN-T5)

Модель `google/flan-t5-large` использует:
- Инструкционное обучение на множестве задач
- Задачи парафразирования из различных датасетов
- Улучшенную способность следовать инструкциям

### Датасеты для суммаризации

#### Русский язык (Gazeta)

**Датасет:** Gazeta Dataset
- **Источник:** Российские новостные сайты
- **Объем:** >90,000 статей с суммариями
- **Формат:** Статья + референтная суммари
- **Характеристики:**
  - Разнообразие тематик
  - Различная длина статей (от 500 до 5000 символов)
  - Профессиональные суммари (не автоматические)

#### Английский язык (CNN/DailyMail)

**Датасет:** CNN/DailyMail Dataset
- **Источник:** CNN и Daily Mail новости
- **Объем:** ~300,000 статей с суммариями
- **Формат:** Статья + референтная суммари
- **Характеристики:**
  - Высокое качество суммарий
  - Разнообразие стилей
  - Актуальные новости

---

## Механизм загрузки моделей

### Lazy Loading (ленивая загрузка)

По умолчанию модели загружаются **только при первом запросе**, что ускоряет старт сервиса и экономит память.

```python
# В config.py
PRELOAD_MODELS=false  # По умолчанию
```

**Процесс загрузки:**

1. **Проверка наличия в кэше:**
   ```python
   model_path = self.models_cache_dir / model_folder
   local_model_exists = model_path.exists() and config_exists and weights_exist
   ```

2. **Загрузка из локального кэша (если есть):**
   ```python
   tokenizer = AutoTokenizer.from_pretrained(str(model_path), local_files_only=True)
   model = T5ForConditionalGeneration.from_pretrained(str(model_path), local_files_only=True)
   ```

3. **Автоматическая загрузка с Hugging Face (если нет локально):**
   ```python
   if settings.auto_download_models:
       tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=str(self.models_cache_dir))
       model = T5ForConditionalGeneration.from_pretrained(model_name, cache_dir=str(self.models_cache_dir))
   ```

4. **Оптимизация памяти:**
   - Перемещение на устройство (CPU/CUDA)
   - Перевод в режим оценки (`model.eval()`)
   - Отключение градиентов (`torch.no_grad()`)
   - Опциональное использование float16 на CUDA

### Preload Models (предзагрузка)

Если `PRELOAD_MODELS=true`, все модели загружаются при старте сервиса:

```python
@asynccontextmanager
async def lifespan(app: FastAPI):
    if settings.preload_models:
        # Загрузка всех моделей последовательно
        processor._load_paraphrase_model('ru')
        processor._load_paraphrase_model('en')
        processor._load_summary_model_ru()
```

**Преимущества:**
- Первый запрос обрабатывается быстрее
- Модели готовы к работе сразу

**Недостатки:**
- Медленный старт сервиса (10-20 минут)
- Больше потребление памяти

### Кэширование моделей

Все модели кэшируются в директории, указанной в `ML_MODEL_CACHE_DIR` (по умолчанию `./models_cache`).

**Структура кэша:**
```
models_cache/
├── rut5-base-paraphraser/
│   ├── config.json
│   ├── pytorch_model.bin
│   ├── tokenizer_config.json
│   └── ...
├── flan-t5-large/
│   ├── config.json
│   ├── model.safetensors
│   └── ...
├── mbart_ru_sum_gazeta/
│   ├── config.json
│   ├── pytorch_model.bin
│   └── ...
└── ...
```

**Преимущества кэширования:**
- Быстрая загрузка при последующих запусках
- Экономия трафика (модели скачиваются один раз)
- Работа в офлайн-режиме (если модели уже загружены)

---

## Принцип работы

### Парафразирование

#### Процесс обработки:

1. **Определение языка:**
   ```python
   language = self._detect_language(text)  # 'ru' или 'en'
   ```

2. **Загрузка соответствующей модели:**
   ```python
   model, tokenizer = self._load_paraphrase_model(language)
   ```

3. **Подготовка промпта:**
   ```python
   if language == 'ru':
       prompt = text  # Для русской модели префикс не нужен
   else:
       prompt = f"paraphrase: {text}"  # Для английской модели нужен префикс
   ```

4. **Токенизация:**
   ```python
   inputs = tokenizer(
       prompt,
       max_length=512,
       truncation=True,
       padding=True,
       return_tensors="pt"
   )
   ```

5. **Генерация:**
   ```python
   outputs = model.generate(
       **inputs,
       max_length=max_length,
       num_beams=num_beams,
       temperature=temperature,
       top_p=top_p,
       early_stopping=True,
       do_sample=True
   )
   ```

6. **Декодирование и постобработка:**
   ```python
   paraphrased = tokenizer.decode(outputs[0], skip_special_tokens=True)
   paraphrased = self._clean_paraphrased_text(paraphrased)  # Удаление артефактов
   ```

#### Очистка результата:

Функция `_clean_paraphrased_text()` удаляет:
- Лишние экранирования кавычек (`\"`)
- Двойные кавычки в начале/конце текста
- Лишние пробелы
- Пробелы перед знаками препинания

### Суммаризация

#### Процесс обработки:

1. **Определение языка:**
   ```python
   language = language or "ru"  # По умолчанию русский
   ```

2. **Загрузка модели:**
   ```python
   model, tokenizer = self._load_summary_model_ru()
   ```

3. **Настройка языка для mBART:**
   ```python
   if hasattr(tokenizer, 'src_lang'):
       tokenizer.src_lang = "ru_RU"
   if hasattr(tokenizer, 'tgt_lang'):
       tokenizer.tgt_lang = "ru_RU"
   ```

4. **Токенизация:**
   ```python
   inputs = tokenizer(
       text,
       max_length=1024,  # Ограничение длины входного текста
       truncation=True,
       padding=True,
       return_tensors="pt"
   )
   ```

5. **Расчет параметров генерации:**
   ```python
   # Преобразование target_length из символов в токены
   max_tokens = int((target_length or 200) * 1.5)
   min_tokens = max(30, int((target_length or 200) * 0.3))
   ```

6. **Генерация суммари:**
   ```python
   summary_ids = model.generate(
       input_ids=inputs["input_ids"],
       max_length=max_tokens,
       min_length=min_tokens,
       num_beams=4,
       early_stopping=True,
       length_penalty=1.2,
       no_repeat_ngram_size=3,
       do_sample=False
   )
   ```

7. **Декодирование и постобработка:**
   ```python
   summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
   summary = self._trim_to_complete_sentence(summary, target_length)
   ```

#### Обрезка до полного предложения:

Функция `_trim_to_complete_sentence()`:
- Ищет последний знак препинания (`.`, `!`, `?`, `…`)
- Обрезает текст до знака препинания
- Избегает обрыва на середине предложения

---

## Конфигурация

### Переменные окружения

Все настройки моделей находятся в файле `.env` или задаются через переменные окружения:

```bash
# Модели парафразирования
PARAPHRASE_MODEL_RU=cointegrated/rut5-base-paraphraser
PARAPHRASE_MODEL_EN=google/flan-t5-large

# Модели суммаризации
SUMMARY_MODEL_RU=IlyaGusev/mbart_ru_sum_gazeta
SUMMARY_MODEL_EN=facebook/bart-large-cnn

# Параметры обработки
ML_MAX_LENGTH=512
ML_TEMPERATURE=0.7
ML_TOP_P=0.9
SUMMARY_THRESHOLD_TOKENS=1800
SUMMARY_CHUNK_SIZE=900
SUMMARY_TARGET_LENGTH=600

# Загрузка моделей
ML_MODEL_CACHE_DIR=./models_cache
ML_DEVICE=cpu  # или cuda
AUTO_DOWNLOAD_MODELS=true
PRELOAD_MODELS=false
```

### Параметры генерации

#### Парафразирование:

- **max_length**: Максимальная длина выходного текста (512 токенов)
- **temperature**: Температура выборки (0.7 - баланс между разнообразием и качеством)
  - Низкие значения (0.3-0.5): более детерминированный, предсказуемый вывод
  - Высокие значения (0.8-1.2): более разнообразный, креативный вывод
- **top_p**: Nucleus sampling (0.9)
  - Учитывает только топ-90% вероятностной массы токенов
- **num_beams**: Количество лучей для beam search (5)
  - Больше = лучше качество, но медленнее

#### Суммаризация:

- **max_length**: Максимальная длина суммари (рассчитывается динамически)
- **min_length**: Минимальная длина суммари (30% от max_length, минимум 50 токенов)
- **num_beams**: Количество лучей (4)
- **length_penalty**: Штраф за длину (1.2)
  - > 1.0: поощряет более короткие суммари
  - < 1.0: поощряет более длинные суммари
- **no_repeat_ngram_size**: Запрет повторения N-грамм (3)
  - Избегает повторений фраз из 3+ слов

---

## Производительность и оптимизация

### Время обработки

**Парафразирование:**
- RUT5 (русский): ~2-5 секунд на 500 символов (CPU)
- FLAN-T5 (английский): ~3-7 секунд на 500 символов (CPU)

**Суммаризация:**
- mBART Gazeta (русский): ~10-30 секунд на статью (CPU)
- BART CNN (английский): ~8-20 секунд на статью (CPU)

### Использование GPU

Для ускорения можно использовать CUDA:

```bash
ML_DEVICE=cuda
```

**Преимущества GPU:**
- Ускорение в 5-10 раз
- Параллельная обработка нескольких запросов

**Требования:**
- NVIDIA GPU с поддержкой CUDA
- CUDA Toolkit
- PyTorch с CUDA поддержкой

### Оптимизация памяти

1. **Float16 на CUDA:**
   ```python
   if device == "cuda" and torch.cuda.is_available():
       model = model.half()  # Уменьшение памяти в 2 раза
   ```

2. **Lazy Loading:**
   - Модели загружаются только при использовании
   - Экономия памяти при старте

3. **Очистка памяти после загрузки:**
   ```python
   import gc
   gc.collect()
   if torch.cuda.is_available():
       torch.cuda.empty_cache()
   ```

4. **Кэширование моделей:**
   - Модели остаются в памяти после первой загрузки
   - Последующие запросы быстрее

### Масштабирование

Для обработки большого объема запросов:

1. **Несколько воркеров:**
   ```bash
   API_WORKERS=4  # Количество воркеров FastAPI
   ```

2. **Асинхронная обработка:**
   - FastAPI поддерживает async/await
   - Параллельная обработка запросов

3. **Кэширование результатов:**
   ```bash
   CACHE_ENABLED=true
   CACHE_TTL=604800  # 7 дней
   ```

---

## Примеры использования

### Парафразирование через API

```python
import requests

response = requests.post(
    "http://localhost:8000/paraphrase",
    json={
        "text": "Исходный текст для парафразирования",
        "max_length": 512,
        "temperature": 0.7,
        "top_p": 0.9,
        "num_beams": 5
    },
    headers={"X-API-Key": "your-api-key"}
)

result = response.json()
print(result["paraphrased"])
```

### Суммаризация через API

```python
import requests

response = requests.post(
    "http://localhost:8000/summarize",
    json={
        "text": "Длинный текст статьи...",
        "target_length": 600,
        "language": "ru"
    },
    headers={"X-API-Key": "your-api-key"}
)

result = response.json()
print(result["summary"])
```

---

## Ограничения и известные проблемы

### Ограничения моделей

1. **Длина входного текста:**
   - Парафразирование: максимум ~512 токенов (~2000 символов)
   - Суммаризация: максимум ~1024 токенов (~4000 символов)
   - Для более длинных текстов требуется разбиение на части

2. **Качество:**
   - Модели могут иногда терять детали
   - Парафразирование может изменить стиль больше, чем ожидалось
   - Суммаризация может пропустить важную информацию

3. **Производительность:**
   - Медленная обработка на CPU
   - Высокое потребление памяти для больших моделей

### Известные проблемы

1. **Медленная первая загрузка:**
   - Решение: использовать PRELOAD_MODELS=true или подождать первого запроса

2. **Высокое потребление памяти:**
   - Решение: использовать float16 на CUDA или уменьшить количество воркеров

3. **Артефакты в выводе:**
   - Решение: функция `_clean_paraphrased_text()` автоматически очищает результат

---

## Заключение

Система ML моделей Phoenix LAB использует современные предобученные модели из Hugging Face для высококачественной обработки текста. Благодаря lazy loading, кэшированию и оптимизации памяти, система работает эффективно как на CPU, так и на GPU.

Все модели автоматически определяют язык и выбирают подходящую модель для обработки, что делает систему удобной для пользователей, работающих с разными языками.

